{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* **Sánchez Martínez Felipe** 6BM1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Descenso por gradiente completo\n",
        "\n",
        "El método de descenso por gradiente es una técnica de optimización utilizada para encontrar el mínimo de una función. Consiste en iterativamente ajustar los parámetros de la función en la dirección opuesta al gradiente de la función de pérdida, multiplicado por una tasa de aprendizaje. Esto permite que los parámetros converjan hacia los valores que minimizan la función de pérdida, lo que es crucial en el entrenamiento de modelos de aprendizaje automático como redes neuronales, donde se busca minimizar el error entre las predicciones del modelo y los valores reales.\n",
        "\n",
        "En este notebook implementaremos el algoritmo completo de descenso por gradiente. Para validar que funciona al final lo probaremos en el entrenamiento de una red neuronal simple. Usaremos como conjunto de datos que esta incluído en el archivo data.csv. \n",
        "\n",
        "@juan1rving\n",
        "\n",
        "Nota: Este notebook requiere que copies los archivos requeridos antes de usar en Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# importamos los paquete necesarios\n",
        "import numpy as np\n",
        "\n",
        "# cargamos datos de ejemplo\n",
        "from data_prep import features, targets, features_test, targets_test\n",
        "\n",
        "n_records, n_features = features.shape\n",
        "last_loss = None\n",
        "\n",
        "# En este ejercicio por propósitos de analizar las salidas utilizaremos la misma semilla para los números aleatorios.\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definimos algunas funciones necesarias\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Sigmoide\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inicialización de los pesos\n",
        "\n",
        "En un principio no queremos tener todos los pesos en cero porque esto generaría en la salida una predicción nula. Por lo tanto, asignaremos los pesos iniciales de forma aleatoria y cercanos a cero. Otra recomendación es escalar los valores aleatorios es dependencia del número de entradas del nodo (n).\n",
        "\n",
        "$$w = rand(1,\\frac{1}{\\sqrt{n}})$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Initialize weights. \n",
        "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Probemos la precisión de la red antes de entrenarla\n",
        "tes_out = sigmoid(np.dot(features_test, weights))\n",
        "predictions = tes_out > 0.5\n",
        "accuracy = np.mean(predictions == targets_test)\n",
        "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n",
        "\n",
        "# La precisión debe ser mala seguramente."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hiperparámetros de la red\n",
        "\n",
        "Los hiperpámetros de la red indican el números de veces que itera el método (épocas-epochs), la taza de aprendizaje (learning rate).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# número de épocas\n",
        "epochs = 50\n",
        "# tasa de aprendizaje\n",
        "learnrate = 0.5"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Descenso por gradiente completo.\n",
        "\n",
        "El algoritmo de descenso por gradiente de forma iterativa cambia el valor de los pesos de tal forma que se disminuya el error. \n",
        "\n",
        "<img src=\"files/despg.png\">\n",
        "\n",
        "En la siguiete celda encontrarás la plantilla del algoritmo. Tu misión, si decides aceptarla, es completar el código faltante para que funcione el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss:  0.25504239792953565\n",
            "Train loss:  0.24954304623126508\n",
            "Train loss:  0.24472612494763205\n",
            "Train loss:  0.24047187993855543\n",
            "Train loss:  0.23668836507565527\n",
            "Train loss:  0.23330487618926452\n",
            "Train loss:  0.23026631913221926\n",
            "Train loss:  0.22752888020147838\n",
            "Train loss:  0.22505688328967757\n",
            "Train loss:  0.2228205840324139\n"
          ]
        }
      ],
      "source": [
        "# Variable para registrar el rendimiento\n",
        "History_loss = []\n",
        "\n",
        "# Algoritmo descenso por gradiente\n",
        "for e in range(epochs):\n",
        "    delta_w = np.zeros(weights.shape)\n",
        "    # Para todos los renglones de ejemplo, asignar a x la entrada, y a y la salida deseada\n",
        "    for x, y in zip(features.values, targets):\n",
        "\n",
        "        # TODO: calcula la predicción de la red\n",
        "        # Tip: NumPy ya tiene una función que calcula el producto punto. Recuerda que también los logits tienen que pasar por la función de activación.\n",
        "        output = sigmoid(np.dot(x, weights))\n",
        "\n",
        "        # TODO: calcula el error\n",
        "        error = y - output\n",
        "\n",
        "        # TODO: calcula el incremento\n",
        "        delta_w += learnrate*error*(sigmoid(np.dot(x, weights)) * (1 - sigmoid(np.dot(x, weights))))*x\n",
        "\n",
        "    # TODO: Actualiza los pesos\n",
        "    m = len(features.values)\n",
        "    weights += (1/m)*delta_w\n",
        "\n",
        "    # Ahora calculemos el error en el conjunto de datos de entrenamiento para registro y visualización\n",
        "    out = sigmoid(np.dot(features, weights))\n",
        "    loss = np.mean((out - targets) ** 2)\n",
        "    History_loss.append(loss)\n",
        "    if e % (epochs / 10) == 0:\n",
        "        if last_loss and last_loss < loss:\n",
        "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
        "        else:\n",
        "            print(\"Train loss: \", loss)\n",
        "        last_loss = loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TODO: Prueba distintas cantidades de épocas y visualiza el resultado. Puedes usar los valores almacenados en el historial de la perdida. Si es que tu método esta entrenando bien el resultado deberá graficarse como la siguiente figura.\n",
        "\n",
        "<img src=\"files/train.png\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO Grafica el error conforme avanzaron las épocas del entrenamiento."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Evaluemos la exactitud de la red\n",
        "\n",
        "Validar una red neuronal es crucial porque permite evaluar su rendimiento en datos independientes, verificando su capacidad para generalizar patrones aprendidos durante el entrenamiento. Mientras que el entrenamiento adapta los pesos de la red para minimizar el error en los datos de entrenamiento, la validación revela si el modelo puede hacer predicciones precisas en datos nuevos. Este proceso ayuda a detectar problemas de sobreajuste o subajuste, garantizando así que la red pueda desempeñarse de manera efectiva en situaciones del mundo real y proporcionar resultados confiables y útiles.\n",
        "\n",
        "$$ Exactitud = \\frac{\\# aciertos}{\\# predicciones} $$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction accuracy: 0.725\n"
          ]
        }
      ],
      "source": [
        "# Cálculo de la precisión\n",
        "\n",
        "tes_out = sigmoid(np.dot(features_test, weights))\n",
        "predictions = tes_out > 0.5\n",
        "accuracy = np.mean(predictions == targets_test)\n",
        "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Que tan bien te fue en los resultados? Seguramente bien. Pero ¿qué pasará si incrementamos las épocas? O ¿qué es lo que pasará si cambiamos la tasa de aprendizaje? \n",
        "\n",
        "Escribe tus conclusiones:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Si incrementamos el número de épocas, debería ir disminuyendo la variable de pérdida de entrenamiento con cada iteración, además de aumentar la precisión del modelo. Por otro lado, esto aumentaría la posibilidad de un sobreajuste en las neuronas.\n",
        "* Si cambiamos la tasa de aprendizaje, podemos hacer que el modelo tenga una convergencia más rápida a un mínimo o máximo local, pero aumenta el riesgo de estancamiento en un mínimo o máximo local. Por otro lado, a menor tasa de aprendizaje, el modelo tratará de converger más lentamente, pero tendrá un mayor rango de exploración para no quedar estancado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusión:\n",
        "\n",
        "La forma matemática de utilizar el descenso por gradiente para la actualización de los pesos de una neurona es muy eficiente, al usar únicamente la derivada para obtener un mejor peso, y puede ajustarse con la tasa de aprendizaje y el número de épocas para mejorar la precisión y disminuir el valor de pérdida de entrenamiento de la red. En cambio, esos mismos parámetros juegan un papel crucial al momento de buscar el mínimo o máximo local de nuestro problema. El uso del descenso por gradiente en sí se puede aplicar a toda la red neuronal, lo que permite que cada neurona actualice sus propios pesos, mejorando así la resolución de problemas más complejos. Sin embargo, en cuanto a la complejidad temporal, esto dependería del número de neuronas en la red, y podría ser menor con técnicas de programación paralela.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "vscode": {
      "interpreter": {
        "hash": "cbad788490f55b163920bee5e9d5e0cba00db5905dc94f4bdbe0011e55bf465f"
      }
    },
    "widgets": {
      "state": {},
      "version": "1.1.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
